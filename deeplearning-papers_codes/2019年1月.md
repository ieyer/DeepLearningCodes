# 深度学习-论文&开源代码

## 2019年1月20日星期日

# Self-Attention Generative Adversarial Networks

论文：https://arxiv.org/abs/1805.08318v1

代码：

Pytorch实现：https://github.com/heykeetae/Self-Attention-GAN

TensorFlow实现：https://github.com/taki0112/Self-Attention-GAN-Tensorflow

 
Abstract：In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.

在这篇论文中，我们提出了自我注意生成对抗网络(SAGAN)，它允许对图像生成任务进行注意力驱动的长期依赖建模。在低分辨率特征图中，传统的卷积GANs只根据空间局部点生成高分辨率细节。在SAGAN中，可以使用来自所有特征位置的线索生成细节。此外，该鉴别器还可以检查图像中较远部分的细节特征是否一致。此外，最近的研究表明，生成器调节影响GAN的性能。利用这一观点，我们将光谱归一化应用于GAN生成器，并发现这改善了训练的动态性。在具有挑战性的ImageNet数据集上，提议的SAGAN实现了最先进的结果，将最佳发布Inception分数从36.8提高到52.52，并将Frechet Inception距离从27.62降低到18.65。注意力层的可视化显示，生成器利用的是与对象形状对应的邻域，而不是固定形状的局部区域。
 


要点：
attention模型：获取全局依赖关系；
Self-attention模型（也称intra-attention）：通过关注同一序列中的所有位置来计算序列中某个位置的响应。
SAGAN在图像的内部表示中有效地找到全局的、长期的依赖关系。
卷积在一个局部邻域内处理信息，因此仅使用卷积层对于在图像中建模长期依赖关系的计算效率很低。
两个技巧：
spectral normalization（论文16）：通过限制各层的谱范数来约束鉴别器的Lipschitz常数，以稳定化gan网络训练。与其他归一化技术相比，光谱归一化不需要额外的超参数调优(将所有权重层的光谱范数设置为1在实践中表现良好)。此外，计算成本也相对较小。谱归一化可以防止参数幅度的增大，避免异常梯度。
two-timescale update rule (TTUR)：训练生成器和判别器使用不同的学习率（using separate learning rates (TTUR) for the generator and the discriminator）
 
评估指标：
Inception score[26]计算条件类分布和边缘类分布之间的KL散度，越高表示图像质量越好。缺陷：它的主要目的是确保模型生成的样本可以被自信地识别为属于一个特定的类，并且模型从许多类生成样本，而不必评估细节的真实性或类内多样性。

Fréchet Inception distance (FID) [8]计算生成的图像与真实图像之间的Wasserstein-2距离，越小越好。在评估生成样本的真实性和变异性时更符合人类的评价。

 
 

# FaceBoxes: A CPU Real-time Face Detector with High Accuracy
https://arxiv.org/abs/1708.05234v4

代码：
Caffe实现：https://github.com/sfzhang15/FaceBoxes

Although tremendous strides have been made in face detection, one of the remaining open challenges is to achieve real-time speed on the CPU as well as maintain high performance, since effective models for face detection tend to be computationally prohibitive. To address this challenge, we propose a novel face detector, named FaceBoxes, with superior performance on both speed and accuracy. Specifically, our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers (RDCL) and the Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable FaceBoxes to achieve real-time speed on the CPU. The MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle faces of various scales. Besides, we propose a new anchor densification strategy to make different types of anchors have the same density on the image, which significantly improves the recall rate of small faces. As a consequence, the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images. Moreover, the speed of FaceBoxes is invariant to the number of faces. We comprehensively evaluate this method and present state-of-the-art detection performance on several face detection benchmark datasets, including the AFW, PASCAL face, and FDDB. Code is available at https://github.com/sfzhang15/FaceBoxes

虽然在人脸检测方面已经取得了巨大进步，但其余的开放性挑战之一是在CPU上实现实时以及保持高性能检测，因为用于面部检测的有效模型往往在计算上代价过高。为了应对这一挑战，我们提出了一种名为FaceBoxes的新型人脸检测器，它在速度和精度方面都具有卓越的性能。具体来说，我们的方法具有轻量级但功能强大的网络结构，包括快速处理卷积层（RDCL）和多尺度卷积层（MSCL）。 RDCL使FaceBoxes能够在CPU上实现实时速度。 MSCL的目的是丰富接收域，并在不同的层上离散锚点来处理不同尺度的面。此外，我们提出了一种新的锚点致密化策略，使不同类型的锚点在图像上具有相同的密度，从而显着提高了小脸部的召回率。因此，所提出的检测器在单CPU核心上以20 FPS运行，在使用GPU用于VGA分辨率图像时以125 FPS运行。而且，FaceBoxes的速度不受面部的数量影响。我们全面评估了这种方法，并在多个人脸检测基准数据集上展示了最先进的检测性能，包括AFW，PASCAL人脸和FDDB。代码可在https://github.com/sfzhang15/FaceBoxes获得。

 
 
 

# Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification

https://arxiv.org/abs/1809.04427v1

代码：https://github.com/longcw/MOTDT

 

Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on large-scale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.

在线多目标跟踪是时间关键型视频分析应用中的一个基本问题。在流行的检测-跟踪框架中，一个主要的挑战是如何将不可靠的检测结果与现有的跟踪联系起来。在本文中，我们建议通过从检测和跟踪的输出中收集候选项来处理不可靠的检测。生成冗余候选项的动机是，在不同的场景中，检测和跟踪可以互补。高置信度的检测结果可以长期防止跟踪漂移，对轨迹的预测可以处理遮挡引起的噪声检测。为了实时地从大量的候选图像中进行最优选择，我们提出了一种基于全卷积神经网络的新的评分函数，该函数在整幅图像上共享了大部分的计算。此外，我们采用深度学习习得的外观表示，这是在大规模的行人重识别数据集上训练的，以提高我们的跟踪识别能力。大量的实验表明，我们的跟踪器在广泛使用的行人跟踪基准上实现了实时和最先进的性能。

补充：
行人重识别（Person re-identification）也称行人再识别，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术。广泛被认为是一个图像检索的子问题。 给定一个监控行人图像，检索跨设备下的该行人图像。旨在弥补目前固定的摄像头的视觉局限，并可与行人检测/行人跟踪技术相结合，可广泛应用于智能视频监控、智能安保等领域。
综述：http://html.rhhz.net/tis/html/201706084.htm

 
# Progressive Growing of GANs for Improved Quality, Stability, and Variation

论文：https://arxiv.org/abs/1710.10196v3

代码（pytorch）：https://github.com/akanimax/pro_gan_pytorch

 
We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.

我们描述了一种新的生成对抗网络训练方法。关键思想是逐步增加生成器和鉴别器:从低分辨率开始，我们添加新的层，随着训练的进展，这些层对越来越精细的细节进行建模。这个训练速度和极大的稳定,让我们产生了前所未有的图像质量,例如,CelebA图像1024 ^ 2。我们还提出了一种简单的方法来增加生成图像的变化，并在无监督的CIFAR10中实现了8.80的无监督评分。此外，我们还描述了几个实现细节，它们对于阻止生成器和判别器之间的不健康竞争非常重要。最后，我们提出了一种新的评价GAN结果的指标，包括图像质量和变化。作为额外的贡献，我们构建了CelebA数据集的高质量版本。
 


# A Neural Algorithm of Artistic Style
论文：https://arxiv.org/abs/1508.06576v2

代码：

TensorFlow：https://github.com/ckmarkoh/neuralart_tensorflow

Keras：https://github.com/titu1994/Neural-Style-Transfer

Caffe：https://github.com/fzliu/style-transfer

In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.

在美术，尤其是绘画中，人类已经掌握了通过在图像的内容和风格之间构成一种复杂的相互作用来创造独特的视觉体验的技巧。到目前为止，这个过程的算法基础是未知的，不存在类似功能的人工系统。然而，在视觉感知的其他关键领域，如物体和人脸识别，最近一种被称为深度神经网络的受生物启发的视觉模型展示了接近人类的表现。在这里，我们介绍了一个基于深度神经网络的人工系统，它可以创建高感知质量的艺术图像。该系统利用神经表示对任意图像的内容和风格进行分离和重组，为艺术图像的创作提供了一种神经算法。此外，考虑到性能优化的人工神经网络与生物视觉之间惊人的相似性，我们的工作为算法理解人类如何创造和感知艺术图像提供了一条道路。
 

